{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3906da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd\n"
     ]
    }
   ],
   "source": [
    "print(\"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7147036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ íŒŒì¼ ìˆ˜: 368\n",
      "Train: 257 / Valid: 55 / Test: 56\n",
      "------------------------------\n",
      "ğŸ“‚ train ë°ì´í„° ë³µì‚¬ ì¤‘...\n",
      "ğŸ“‚ valid ë°ì´í„° ë³µì‚¬ ì¤‘...\n",
      "ğŸ“‚ test ë°ì´í„° ë³µì‚¬ ì¤‘...\n",
      "------------------------------\n",
      "âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: C:\\Dev\\KorailWheel\\data\\split_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì •\n",
    "# ì›ë³¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ë¡œ\n",
    "source_root = r'C:\\Dev\\KorailWheel\\data\\train' \n",
    "source_img_dir = os.path.join(source_root, 'images')\n",
    "source_lbl_dir = os.path.join(source_root, 'labels')\n",
    "\n",
    "# ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ (ìƒˆë¡œ ìƒì„±ë¨)\n",
    "output_root = r'C:\\Dev\\KorailWheel\\data\\split_dataset'\n",
    "\n",
    "# 2. ë¹„ìœ¨ ì„¤ì • (7 : 1.5 : 1.5)\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# 3. ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„± í•¨ìˆ˜\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# í´ë” êµ¬ì¡° ìƒì„±: images/train, labels/train ë“±\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    create_dir(os.path.join(output_root, 'images', split))\n",
    "    create_dir(os.path.join(output_root, 'labels', split))\n",
    "\n",
    "# 4. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ë° ì…”í”Œ\n",
    "# ì´ë¯¸ì§€ í™•ì¥ì (í•„ìš”ì‹œ ì¶”ê°€)\n",
    "image_exts = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "files = [f for f in os.listdir(source_img_dir) if os.path.splitext(f)[1].lower() in image_exts]\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì • (ì„ íƒ ì‚¬í•­)\n",
    "random.seed(42)\n",
    "random.shuffle(files)\n",
    "\n",
    "# 5. ë¶„í•  ì§€ì  ê³„ì‚°\n",
    "total_count = len(files)\n",
    "train_idx = int(total_count * train_ratio)\n",
    "valid_idx = train_idx + int(total_count * valid_ratio)\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ìŠ¬ë¼ì´ì‹±\n",
    "train_files = files[:train_idx]\n",
    "valid_files = files[train_idx:valid_idx]\n",
    "test_files = files[valid_idx:]\n",
    "\n",
    "print(f\"ì´ íŒŒì¼ ìˆ˜: {total_count}\")\n",
    "print(f\"Train: {len(train_files)} / Valid: {len(valid_files)} / Test: {len(test_files)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 6. íŒŒì¼ ë³µì‚¬ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def copy_files(file_list, split_name):\n",
    "    print(f\"ğŸ“‚ {split_name} ë°ì´í„° ë³µì‚¬ ì¤‘...\")\n",
    "    for file_name in file_list:\n",
    "        # íŒŒì¼ëª…ê³¼ í™•ì¥ì ë¶„ë¦¬\n",
    "        name, ext = os.path.splitext(file_name)\n",
    "        \n",
    "        # ì†ŒìŠ¤ ê²½ë¡œ\n",
    "        src_img = os.path.join(source_img_dir, file_name)\n",
    "        src_lbl = os.path.join(source_lbl_dir, name + '.txt')\n",
    "        \n",
    "        # íƒ€ê²Ÿ ê²½ë¡œ\n",
    "        dst_img = os.path.join(output_root, 'images', split_name, file_name)\n",
    "        dst_lbl = os.path.join(output_root, 'labels', split_name, name + '.txt')\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ë³µì‚¬\n",
    "        shutil.copy2(src_img, dst_img)\n",
    "        \n",
    "        # ë¼ë²¨ ë³µì‚¬ (ë¼ë²¨ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)\n",
    "        if os.path.exists(src_lbl):\n",
    "            shutil.copy2(src_lbl, dst_lbl)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "copy_files(train_files, 'train')\n",
    "copy_files(valid_files, 'valid')\n",
    "copy_files(test_files, 'test')\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b49fc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì´ë¯¸ì§€ ê°œìˆ˜: 368ì¥\n",
      "- 'img'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼(í›„ë³´êµ°): 268ì¥\n",
      "- ê·¸ ì™¸ íŒŒì¼(Train í™•ì •): 100ì¥\n",
      "\n",
      "[ë°ì´í„°ì…‹ ë¶„í•  ê²°ê³¼]\n",
      "- Train : 268ì¥ (ë‚˜ë¨¸ì§€ imgíŒŒì¼ + ê·¸ ì™¸ íŒŒì¼)\n",
      "- Valid : 50ì¥ (imgíŒŒì¼ ì¤‘ ëœë¤ 50ì¥)\n",
      "- Test  : 50ì¥ (imgíŒŒì¼ ì¤‘ ëœë¤ 50ì¥)\n",
      "\n",
      "Processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:00<00:00, 272.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing valid data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 268.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 301.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'data' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # ì§„í–‰ìƒí™© í‘œì‹œìš© (ì—†ìœ¼ë©´ pip install tqdm)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ì„¤ì • (ê²½ë¡œ í™•ì¸ í•„ìˆ˜)\n",
    "# ==========================================\n",
    "BASE_DIR = Path(\"C:/Dev/KorailWheel/data\")  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ\n",
    "SOURCE_DIR = BASE_DIR / \"total\"        # ì›ë³¸ ì†ŒìŠ¤ í´ë”\n",
    "DEST_DIR = BASE_DIR / \"data\"           # ë¶„í• ëœ ë°ì´í„°ê°€ ì €ì¥ë  í´ë”\n",
    "\n",
    "# ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)\n",
    "random.seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 2. íŒŒì¼ ëª©ë¡ ì½ê¸° ë° ë¶„ë¥˜\n",
    "# ==========================================\n",
    "image_dir = SOURCE_DIR / \"images\"\n",
    "label_dir = SOURCE_DIR / \"labels\"\n",
    "\n",
    "# ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì ëª©ë¡\n",
    "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "# ì „ì²´ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "all_images = [\n",
    "    f for f in image_dir.iterdir() \n",
    "    if f.suffix.lower() in valid_extensions and f.is_file()\n",
    "]\n",
    "\n",
    "print(f\"ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {len(all_images)}ì¥\")\n",
    "\n",
    "# ê·¸ë£¹ ë¶„ë¥˜\n",
    "# 'img'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ (ê³ í’ˆì§ˆ/ì˜¤í† ë¼ë²¨ ë³´ì •ë³¸ ì¶”ì •) -> Valid/Test í›„ë³´\n",
    "candidates = [img for img in all_images if img.name.lower().startswith(\"img\")]\n",
    "\n",
    "# ê·¸ ì™¸ íŒŒì¼ (ìˆ˜ë™ ë¼ë²¨ë§ ë“±) -> ë¬´ì¡°ê±´ Train\n",
    "others = [img for img in all_images if not img.name.lower().startswith(\"img\")]\n",
    "\n",
    "print(f\"- 'img'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼(í›„ë³´êµ°): {len(candidates)}ì¥\")\n",
    "print(f\"- ê·¸ ì™¸ íŒŒì¼(Train í™•ì •): {len(others)}ì¥\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. ë°ì´í„° ë¶„í•  (Split)\n",
    "# ==========================================\n",
    "# í›„ë³´êµ°ì´ ìµœì†Œ 100ì¥ì€ ë„˜ì–´ì•¼ í•¨ (Valid 50 + Test 50)\n",
    "if len(candidates) < 100:\n",
    "    raise ValueError(f\"ì˜¤ë¥˜: 'img'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì´ 100ì¥ ë¯¸ë§Œì…ë‹ˆë‹¤. ({len(candidates)}ì¥)\")\n",
    "\n",
    "# ëœë¤ ì…”í”Œ\n",
    "random.shuffle(candidates)\n",
    "\n",
    "# 50ì¥ì”© ë½‘ê¸°\n",
    "valid_set = candidates[:50]\n",
    "test_set = candidates[50:100]\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ Trainìœ¼ë¡œ\n",
    "train_from_candidates = candidates[100:]\n",
    "train_set = train_from_candidates + others\n",
    "\n",
    "print(\"\\n[ë°ì´í„°ì…‹ ë¶„í•  ê²°ê³¼]\")\n",
    "print(f\"- Train : {len(train_set)}ì¥ (ë‚˜ë¨¸ì§€ imgíŒŒì¼ + ê·¸ ì™¸ íŒŒì¼)\")\n",
    "print(f\"- Valid : {len(valid_set)}ì¥ (imgíŒŒì¼ ì¤‘ ëœë¤ 50ì¥)\")\n",
    "print(f\"- Test  : {len(test_set)}ì¥ (imgíŒŒì¼ ì¤‘ ëœë¤ 50ì¥)\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. íŒŒì¼ ë³µì‚¬ ì‹¤í–‰\n",
    "# ==========================================\n",
    "def copy_files(file_list, split_name):\n",
    "    # ì €ì¥í•  ê²½ë¡œ ìƒì„± (ì˜ˆ: data/train/images)\n",
    "    save_img_dir = DEST_DIR / split_name / \"images\"\n",
    "    save_lbl_dir = DEST_DIR / split_name / \"labels\"\n",
    "    \n",
    "    save_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nProcessing {split_name} data...\")\n",
    "    \n",
    "    for img_path in tqdm(file_list):\n",
    "        # 1. ì´ë¯¸ì§€ ë³µì‚¬\n",
    "        shutil.copy2(img_path, save_img_dir / img_path.name)\n",
    "        \n",
    "        # 2. ë¼ë²¨ ë³µì‚¬ (ì§ì´ ë§ëŠ” txt íŒŒì¼ ì°¾ê¸°)\n",
    "        label_name = img_path.stem + \".txt\"\n",
    "        label_src = label_dir / label_name\n",
    "        \n",
    "        if label_src.exists():\n",
    "            shutil.copy2(label_src, save_lbl_dir / label_name)\n",
    "        else:\n",
    "            # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš° ê²½ê³  (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)\n",
    "            # print(f\"Warning: Label not found for {img_path.name}\")\n",
    "            pass\n",
    "\n",
    "# ê¸°ì¡´ data í´ë”ê°€ ìˆë‹¤ë©´ ì‚­ì œ í›„ ì¬ìƒì„± (ê¹¨ë—í•œ ìƒíƒœ ìœ ì§€)\n",
    "if DEST_DIR.exists():\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "    print(f\"\\nê¸°ì¡´ {DEST_DIR} í´ë”ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë³µì‚¬ ìˆ˜í–‰\n",
    "copy_files(train_set, \"train\")\n",
    "copy_files(valid_set, \"valid\")\n",
    "copy_files(test_set, \"test\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! 'data' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10af227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Saved splits: C:\\Dev\\KorailWheel\\data\\patch\\splits_src.csv\n",
      "Saved manifest: C:\\Dev\\KorailWheel\\data\\patch\\manifest.csv\n",
      "split\n",
      "test     25\n",
      "train    85\n",
      "valid    21\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# build_patch_lib.py\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import ROOT\n",
    "\n",
    "# -------------------------\n",
    "# Settings (ONLY here)\n",
    "# -------------------------\n",
    "SRC_DIR = ROOT / \"data\" / \"patch_all\"          # images/, labels/\n",
    "OUT_DIR = ROOT / \"data\" / \"patch\"              # train/valid/test ìƒì„±\n",
    "\n",
    "SEED = 0\n",
    "RATIO_TRAIN = 0.70\n",
    "RATIO_VALID = 0.15\n",
    "RATIO_TEST  = 0.15\n",
    "\n",
    "# patch crop ì˜µì…˜\n",
    "MARGIN = 16                 # ì¸ìŠ¤í„´ìŠ¤ bbox ë°”ê¹¥ ì—¬ìœ  í”½ì…€\n",
    "MIN_MASK_AREA = 120         # ë„ˆë¬´ ì‘ì€ ì¸ìŠ¤í„´ìŠ¤(ë…¸ì´ì¦ˆ) ì œê±°\n",
    "APPROX_EPS = 2.0            # contour í´ë¦¬ê³¤ ë‹¨ìˆœí™” ì •ë„(í”½ì…€)\n",
    "\n",
    "# í´ë˜ìŠ¤ í•„í„°(í•„ìš”ì‹œ)\n",
    "# Noneì´ë©´ ëª¨ë“  í´ë˜ìŠ¤ ì¶”ì¶œ, ì˜ˆ: {0} ì²˜ëŸ¼ ì§€ì •í•˜ë©´ í•´ë‹¹ clsë§Œ ì¶”ì¶œ\n",
    "KEEP_CLASSES = None\n",
    "\n",
    "# ì¶œë ¥ í¬ë§·\n",
    "IMG_EXT = \".png\"            # crop ì €ì¥ í™•ì¥ì\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "def _img_list(folder: Path):\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    return sorted([p for p in folder.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "\n",
    "def _read_yolo_seg_instances(label_path: Path):\n",
    "    \"\"\"\n",
    "    YOLOv8-seg line: cls xc yc w h x1 y1 x2 y2 ... (normalized)\n",
    "    return: list[(cls:int, poly_norm:(N,2) float32)]\n",
    "    \"\"\"\n",
    "    if not label_path.exists():\n",
    "        return []\n",
    "    txt = label_path.read_text(encoding=\"utf-8\").strip()\n",
    "    if not txt:\n",
    "        return []\n",
    "\n",
    "    inst = []\n",
    "    for line in txt.splitlines():\n",
    "        p = line.strip().split()\n",
    "        if len(p) < 6:\n",
    "            continue\n",
    "        cls = int(float(p[0]))\n",
    "        coords = np.array(list(map(float, p[5:])), dtype=np.float32)\n",
    "        if coords.size >= 6:\n",
    "            poly = coords.reshape(-1, 2)\n",
    "            inst.append((cls, poly))\n",
    "    return inst\n",
    "\n",
    "\n",
    "def _poly_to_mask(poly_norm: np.ndarray, w: int, h: int):\n",
    "    m = np.zeros((h, w), dtype=np.uint8)\n",
    "    pts = poly_norm.copy()\n",
    "    pts[:, 0] = np.clip(pts[:, 0] * w, 0, w - 1)\n",
    "    pts[:, 1] = np.clip(pts[:, 1] * h, 0, h - 1)\n",
    "    pts = pts.astype(np.int32)\n",
    "    if pts.shape[0] >= 3:\n",
    "        cv2.fillPoly(m, [pts], 1)\n",
    "    return m\n",
    "\n",
    "\n",
    "def _crop_with_margin(img, mask01, margin):\n",
    "    ys, xs = np.where(mask01 > 0)\n",
    "    if ys.size == 0:\n",
    "        return None\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "\n",
    "    H, W = mask01.shape\n",
    "    x0 = max(0, int(x0) - margin)\n",
    "    y0 = max(0, int(y0) - margin)\n",
    "    x1 = min(W - 1, int(x1) + margin)\n",
    "    y1 = min(H - 1, int(y1) + margin)\n",
    "\n",
    "    crop_img = img[y0:y1+1, x0:x1+1].copy()\n",
    "    crop_m   = mask01[y0:y1+1, x0:x1+1].copy()\n",
    "    return crop_img, crop_m, x0, y0\n",
    "\n",
    "\n",
    "def _mask_to_yolo_seg_line(mask01_crop, cls_id):\n",
    "    \"\"\"\n",
    "    crop ë§ˆìŠ¤í¬ì—ì„œ contour ê¸°ë°˜ìœ¼ë¡œ (bbox + polygon) YOLOv8-seg ë¼ì¸ ìƒì„±\n",
    "    return: str or None\n",
    "    \"\"\"\n",
    "    m = (mask01_crop > 0).astype(np.uint8)\n",
    "    cnts, _ = cv2.findContours(m, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not cnts:\n",
    "        return None\n",
    "    cnt = max(cnts, key=cv2.contourArea)\n",
    "    if cv2.contourArea(cnt) < MIN_MASK_AREA:\n",
    "        return None\n",
    "\n",
    "    # polygon ë‹¨ìˆœí™”\n",
    "    eps = APPROX_EPS\n",
    "    approx = cv2.approxPolyDP(cnt, eps, True)\n",
    "    if approx.shape[0] < 3:\n",
    "        return None\n",
    "\n",
    "    H, W = m.shape[:2]\n",
    "    pts = approx.reshape(-1, 2).astype(np.float32)\n",
    "    pts[:, 0] = np.clip(pts[:, 0], 0, W - 1)\n",
    "    pts[:, 1] = np.clip(pts[:, 1], 0, H - 1)\n",
    "\n",
    "    # bbox from contour\n",
    "    x, y, bw, bh = cv2.boundingRect(cnt)\n",
    "    xc = (x + bw / 2) / W\n",
    "    yc = (y + bh / 2) / H\n",
    "    bw_n = bw / W\n",
    "    bh_n = bh / H\n",
    "\n",
    "    # polygon normalized\n",
    "    pts_n = pts.copy()\n",
    "    pts_n[:, 0] = pts_n[:, 0] / W\n",
    "    pts_n[:, 1] = pts_n[:, 1] / H\n",
    "\n",
    "    # line\n",
    "    vals = [f\"{cls_id:d}\", f\"{xc:.6f}\", f\"{yc:.6f}\", f\"{bw_n:.6f}\", f\"{bh_n:.6f}\"]\n",
    "    for (px, py) in pts_n:\n",
    "        vals.append(f\"{px:.6f}\")\n",
    "        vals.append(f\"{py:.6f}\")\n",
    "    return \" \".join(vals)\n",
    "\n",
    "\n",
    "def _make_splits(src_stems, seed, r_tr, r_va, r_te):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    stems = np.array(sorted(list(src_stems)))\n",
    "    rng.shuffle(stems)\n",
    "\n",
    "    n = len(stems)\n",
    "    n_tr = int(round(n * r_tr))\n",
    "    n_va = int(round(n * r_va))\n",
    "    n_te = n - n_tr - n_va\n",
    "\n",
    "    tr = set(stems[:n_tr])\n",
    "    va = set(stems[n_tr:n_tr + n_va])\n",
    "    te = set(stems[n_tr + n_va:])\n",
    "\n",
    "    return tr, va, te\n",
    "\n",
    "\n",
    "def main():\n",
    "    img_dir = SRC_DIR / \"images\"\n",
    "    lab_dir = SRC_DIR / \"labels\"\n",
    "\n",
    "    imgs = _img_list(img_dir)\n",
    "    # \"ë¼ë²¨ ìˆëŠ” ì›ë³¸\"ë§Œ split ê¸°ì¤€ìœ¼ë¡œ ì“°ëŠ” ê²Œ ì•ˆì •ì \n",
    "    labeled = []\n",
    "    for p in imgs:\n",
    "        if (lab_dir / f\"{p.stem}.txt\").exists():\n",
    "            labeled.append(p)\n",
    "\n",
    "    src_stems = {p.stem for p in labeled}\n",
    "    tr_set, va_set, te_set = _make_splits(src_stems, SEED, RATIO_TRAIN, RATIO_VALID, RATIO_TEST)\n",
    "\n",
    "    def _split_of(stem):\n",
    "        if stem in tr_set: return \"train\"\n",
    "        if stem in va_set: return \"valid\"\n",
    "        return \"test\"\n",
    "\n",
    "    # output dirs\n",
    "    for sp in [\"train\", \"valid\", \"test\"]:\n",
    "        (OUT_DIR / sp / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "        (OUT_DIR / sp / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "        (OUT_DIR / sp / \"masks\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # save split mapping (src image ê¸°ì¤€)\n",
    "    split_rows = []\n",
    "    for stem in sorted(src_stems):\n",
    "        split_rows.append({\"src_stem\": stem, \"split\": _split_of(stem)})\n",
    "    pd.DataFrame(split_rows).to_csv(OUT_DIR / \"splits_src.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # extract instances\n",
    "    manifest = []\n",
    "    for img_path in labeled:\n",
    "        stem = img_path.stem\n",
    "        split = _split_of(stem)\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        H, W = img.shape[:2]\n",
    "\n",
    "        inst = _read_yolo_seg_instances(lab_dir / f\"{stem}.txt\")\n",
    "        if KEEP_CLASSES is not None:\n",
    "            inst = [(c, p) for (c, p) in inst if c in KEEP_CLASSES]\n",
    "\n",
    "        for k, (cls_id, poly_norm) in enumerate(inst):\n",
    "            mask01 = _poly_to_mask(poly_norm, W, H)\n",
    "            area = int(mask01.sum())\n",
    "            if area < MIN_MASK_AREA:\n",
    "                continue\n",
    "\n",
    "            cropped = _crop_with_margin(img, mask01, MARGIN)\n",
    "            if cropped is None:\n",
    "                continue\n",
    "            crop_img, crop_m, x0, y0 = cropped\n",
    "\n",
    "            yolo_line = _mask_to_yolo_seg_line(crop_m, cls_id)\n",
    "            if yolo_line is None:\n",
    "                continue\n",
    "\n",
    "            out_stem = f\"{stem}_i{k:02d}_c{cls_id}\"\n",
    "            out_img = OUT_DIR / split / \"images\" / f\"{out_stem}{IMG_EXT}\"\n",
    "            out_lab = OUT_DIR / split / \"labels\" / f\"{out_stem}.txt\"\n",
    "            out_msk = OUT_DIR / split / \"masks\"  / f\"{out_stem}.png\"\n",
    "\n",
    "            cv2.imwrite(str(out_img), crop_img)\n",
    "            cv2.imwrite(str(out_msk), (crop_m > 0).astype(np.uint8) * 255)\n",
    "            out_lab.write_text(yolo_line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "            manifest.append({\n",
    "                \"split\": split,\n",
    "                \"src_image\": str(img_path),\n",
    "                \"src_stem\": stem,\n",
    "                \"inst_idx\": k,\n",
    "                \"cls\": cls_id,\n",
    "                \"area\": area,\n",
    "                \"crop_x0\": int(x0),\n",
    "                \"crop_y0\": int(y0),\n",
    "                \"crop_w\": int(crop_img.shape[1]),\n",
    "                \"crop_h\": int(crop_img.shape[0]),\n",
    "                \"out_image\": str(out_img),\n",
    "                \"out_mask\": str(out_msk),\n",
    "                \"out_label\": str(out_lab),\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(manifest).to_csv(OUT_DIR / \"manifest.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"DONE\")\n",
    "    print(\"Saved splits:\", OUT_DIR / \"splits_src.csv\")\n",
    "    print(\"Saved manifest:\", OUT_DIR / \"manifest.csv\")\n",
    "    df = pd.DataFrame(manifest)\n",
    "    if len(df):\n",
    "        print(df.groupby(\"split\").size())\n",
    "    else:\n",
    "        print(\"No instances extracted. Check labels/classes/min_area.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813874d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDev\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKorailWheel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mst1_roi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "base = Path(r\"C:\\Dev\\KorailWheel\\data\\st1_roi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c272a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wheel_id_quick\u001b[39m(p: \u001b[43mPath\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# íŒŒì¼ëª… ê·œì¹™ì— ë§ê²Œ ìˆ˜ì •: ê¸°ë³¸ì€ ì²« '_' ì „ê¹Œì§€\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     s \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m s \u001b[38;5;28;01melse\u001b[39;00m s\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "def _wheel_id_quick(p: Path):\n",
    "    # íŒŒì¼ëª… ê·œì¹™ì— ë§ê²Œ ìˆ˜ì •: ê¸°ë³¸ì€ ì²« '_' ì „ê¹Œì§€\n",
    "    s = p.stem\n",
    "    return s.split(\"_\")[0] if \"_\" in s else s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44354799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_leakage():\n",
    "    \n",
    "    sets = {}\n",
    "    for sp in [\"train\", \"valid\", \"test\"]:\n",
    "        imgs = _img_list(base / sp / \"images\")\n",
    "        sets[sp] = set(_wheel_id_quick(p) for p in imgs)\n",
    "\n",
    "    inter_tv = sets[\"train\"] & sets[\"valid\"]\n",
    "    inter_tt = sets[\"train\"] & sets[\"test\"]\n",
    "    inter_vt = sets[\"valid\"] & sets[\"test\"]\n",
    "\n",
    "    print(\"train wheels:\", len(sets[\"train\"]))\n",
    "    print(\"valid wheels:\", len(sets[\"valid\"]))\n",
    "    print(\"test  wheels:\", len(sets[\"test\"]))\n",
    "    print(\"overlap train-valid:\", len(inter_tv))\n",
    "    print(\"overlap train-test :\", len(inter_tt))\n",
    "    print(\"overlap valid-test :\", len(inter_vt))\n",
    "    if len(inter_tv)+len(inter_tt)+len(inter_vt) > 0:\n",
    "        print(\"LEAKAGE: wheel_id overlap exists\")\n",
    "    else:\n",
    "        print(\"OK: no wheel_id overlap\")\n",
    "\n",
    "# check_leakage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WheelScan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
